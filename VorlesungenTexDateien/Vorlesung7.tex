\documentclass[../VorlesungMaster.tex]{subfiles}

%Vorlesung 7
%\section{Lineare Regressionsmodelle}
\section{Statistische Modelle}
Lineare Regressionsmodelle:
\begin{itemize}
	\item Einfache lineare Regressionsmodelle
	\item Multivariate lineare Regressionsmodelle
	\item Voraussetzungen für lineare Regressionsmodelle
	\item Generalisierte lineare Regressionsmodelle (GLM)
	\item Auswertung von Expressionsdaten
\end{itemize}
Molekularbiologische Hochdurchsatzdaten:\\
\begin{center}
	\begin{tikzpicture}
	\node[fill=white,
	thick,
	draw,
	minimum height=3.5cm,
	minimum width=1.5cm
	](n1){};
	\node[left=0cm of n1](t1){\# Elemente $\downarrow$};
	\node[above=0cm of n1](t2){$\rightarrow$ \# Sample};
	\end{tikzpicture}
\end{center}

\#Elemente $>>$ \#samples\\
\begin{itemize}
	\item Genom $\rightarrow$ SNP
	\item Epigenom
	\item \underline{Transkriptom} $\rightarrow$ RNA Content einer Zelle
\end{itemize}

\begin{center}
	\begin{tikzpicture}
	\node[fill=white,
	thick,
	draw,
	minimum height=0.5cm,
	minimum width=1.0cm,
	](n1){};
	\node[fill=white,
	right=1cm of n1,
	thick,
	draw,
	minimum height=0.5cm,
	minimum width=1.0cm,
	](n2){};
	\node[fill=white,
	right=1cm of n2,
	thick,
	draw,
	minimum height=0.5cm,
	minimum width=1.0cm,
	](n3){};
	\node[fill=white,
	right=1cm of n3,
	thick,
	draw,
	minimum height=0.5cm,
	minimum width=1.0cm,
	](n4){};
	
	\node[below=1cm of n1,xshift=5cm](text1){pre-m-RNA $\rightarrow$ ncRNA $\rightarrow$ mRNA $\rightarrow$ Protein};
	\node[below=0.1cm of n4](text2){Exons};
	\node[right=0.5cm of n4](text3){DNA};
	
	\draw [] (-1,0) edge (n1);
	\draw [] (n1) edge (n2);
	\draw [] (n2) edge (n3);
	\draw [] (n3) edge (n4);
	\draw [] (7,0) edge (n4);
	
	\draw [->,thick] (n1) edge (text1.west);
	
	
	\end{tikzpicture}
\end{center}


\underline{Ziel}: Messen der Werte einer Zielvariablen in Abhängigkeit von unabhängigen Variablen (sog. Kovariaten) \\
\underline{Def}: Ein statistisches Model stellt eine Zielvariable $Y$ in Beziehung zu \underline{einer} oder mehreren Kovariaten (Kovariablen). \\
Zielvariable = Modell(Kovariate) + Fehler \\
\[ Y = f(X) + \epsilon \]
$Y$: Zielvariable oder auch abhängige Variable \\
$X$: Kovariaten oder auch unabhängige Variable \\
$f(X)$: Unbekannte Funktion, die den systematischen Effekt von $X$ auf $Y$ modelliert \\
$\epsilon$: Zufälliger Fehler. Gibt den Anteil der Varianz von $Y$ an, der \underline{nicht} durch $f(X)$ erklärt werden kann. \\
$\Rightarrow$ Statistische Modelle zerlegen die Zielvariable in einen systematischen ($f(X)$) und in einen zufälligen Teil ($\epsilon$). \\

\subsection{Anwendungen von statistischen Modellen}
Inferenz: Ziel ist es die Art des Zusammenhanges zwischen $Y$ und $X$ zu verstehen. Genauer: Wie ändert sich $Y$ als Funktion von $X$?\\
Vorhersage: Ziel ist es, den Wert von $Y$ so genau wie möglich vorherzusagen. Es ist nicht von primären Interesse die (exakte) Form von $f(X)$ zu kennen. \\

\subsection{Schätzen der Funktion $f(X)$}
$f(X)$ wird mittels einer statischen Lernmethode von einer Menge von Trainingsdaten geschätzt. Die geschätzte Funktion wird mit $\hat{f}(X)$ angegeben.
\[ Y = \hat{f}(X) + \epsilon \]
für Trainingsdaten $(X,Y) = \{(x_1,y_1),...,(x_n,y_n)\}$ n Beobachtungen.

\subsection{Klassifikation statistischer Lernmethoden}
\underline{Parametrische Methoden}: Für die Funktion $f(X)$ wird eine bestimmte Form angenommen.
\begin{enumerate}
	\item Anzahl der Kovariabln wird vorab festegelegt oder mittels Verfahren der Modellselektion ausgewählt.
	\item Es werden die Gewichte für die Kovariablen geschätzt
\end{enumerate}
Nachteil: 
\begin{enumerate}
	\item weniger flexibel
	\item Gewähltes Modell entspricht oft nicht der wahren Form des Zusammenhanges
\end{enumerate}
Vorteil:
\begin{enumerate}
	\item Da nur Gewichte für Kovariablen gelernt werden müssen, reicht eine geringe Stichprobengröße aus
\end{enumerate}

\underline{Nicht-parametrische Methoden}: Es wird keine bestimmte Form für $f(X)$ vorab angenommen.
Es muß die Form und die Parameter für eine beliebig komplexe Funktion $f(X)$ anhand der Trainingsdaten geschätzt werden.
Vorteil:
\begin{enumerate}
	\item Modelle sind sehr flexibel
\end{enumerate}

Nachteil:
\begin{enumerate}
	\item oft weniger gut interpretierbar
	\item oft ein großer Stichprobenumfang notwendig
\end{enumerate}

\section{Lineare Regressionsmodelle}
Als Form für $f(X)$ wird ein (annähernd) linearer Zusammenhang angenommen. Zufallsvariable $Y$ nimmt dabei quantitative Werte an. 
Kovariablen $X_i$ können quantitative oder qualitative Werte annehmen.

\subsection{Einfaches lineares Regressionsmodel}
\underline{Def.} Statistisches Modell, was den Wert der Zielvariablen $Y$ auf Basis der Werte einer einzigen Kovariable $X$, unter der Annahme eines linearen Zusammenhangs modelliert.

\[ Y = \underbrace{\beta_0 + \beta_1}_{Koeffizienten} + \epsilon \]
$\beta_0$: Mittelwert von $Y$, falls es keinen Zusammenhang gibt, sonst Schnittpunkt mit $y-Achse$. \\
$\beta_1$: \underline{Effekt} der Kovariablen $X$ auf $Y$ an, also den Anstieg in $Y$, wenn $X$ sich um eine Einheit erhöht. (Regressionskoeffizient)\\
$\epsilon$: Fehler $\epsilon \sim N(0,\sigma^2)$ Anteil von $Y$, der nicht durch $\beta_0 + \beta_1 X$ erklärt werden kann.

\subsection{Annahmen für zufällige Störgrößen}
\begin{enumerate}
	\item Alle Stätungen haben die gleiche Varianz (Homoskedastizität) $Var(\epsilon_i)= \sigma^2$
	\item Alle Störungen sind um 0 verteilt \\ $E(\epsilon_i)=0$ \\ $\Rightarrow$ Einflüsse der Störgrößen heben sich im Mittel auf, d.h. haben keinen systematischen Einflus auf Y
	\item Störgrößen sind unabhängig untereinander: \\ $Cor(\epsilon_i, \epsilon_j) = 0$
\end{enumerate}

\subsection{Varianzdekomposition (grafische Darstellung)}
\begin{tikzpicture}
\begin{axis}[%
scatter/classes={%
	a={mark=o,draw=gray},
	b={mark=x, draw=gray}},
xlabel={x},
ylabel={y},
xticklabels={,,},
yticklabels={,,}
]
\addplot[scatter,only marks,%
scatter src=explicit symbolic]%
table[meta=label] {
	x y label
	10 12 a
	11 9 a
	12 11 a
	12.3 17 a
	13 14 a
	14 13 a
	14.3 8 a
	15 16 a
	16 17 a
	16.5 19 a
	17 16 a
	18 16 a
	18.2 23 a
	18.4 13 a
	19 21 a
	20 21 a
	21 20 a
	22 24 a
	22.3 30 a
	23 23 a
	24 24 a
	24.2 17 a
	25 26 a
	25 26 b
	25.4 31 a
	26 26.8 a
	27 26 a
	27.8 33 a
	28 27 a
	29 31 a
	30 30 a
	30 31 a
};
\addplot [thick] coordinates { (9,12) (31,27.5) };
\addplot [thick, red] coordinates { (9,20.83) (31,20.83) };

\addplot [gray] coordinates { (25,20.83) (25,26) };
\end{axis}

\node[red] at (rel axis cs:1.4,0.6){$\bar{y}$ (Mittelwert von $y$)};

\node[] at (rel axis cs:0.85,0.63){$(\hat{y_i},x_i)$};
\node[] at (rel axis cs:0.8,0.72){$\epsilon_i$};
\node[gray] at (rel axis cs:0.684,0.78){$(y_i,x_i)$};

\end{tikzpicture}


\[ \underbrace{\sum\limits_{i=1}^n (y_i - \bar{y})^2}_{\text{TSS (total sum of squares)}} = \underbrace{\sum\limits_{i=1}^n (\hat{y}_i - \bar{y})^2}_{\text{erklärbare Varianz}} + \underbrace{\sum\limits_{i=1}^n (\hat{y}_i - y)^2}_{\text{nicht erklärbare Varianz (RSS)} \epsilon_i = \hat{y}_i -y_i} \]

\subsection{Schätzung der Parameter $\beta_0$ und $\beta_1$}
Methode der kleinsten Quadrate: Reduziere Differenz zwischen Werten der Zielvariablen $y_i$ und den vorhergesagten Werten $\hat{y}_i$ für alle Beobachtungen $n$

\[ RSS = \sum\limits_{i=1}^n (y_i -\hat{y}_i)^2 \rightarrow min \]
\[ = \sum\limits_{i=1}^n (y_i -\hat{\beta_0} - \hat{\beta_1} x_i)^2 \rightarrow min \]
\[ \hat{\beta_1} = \frac{\sum\limits_{i=1}^n(x_i - \bar{x})(y_i - \bar{y})}{\sum\limits_{i=1}^n(x_i - \bar{x})} = 
\frac{Cor(X,Y)}{Var(X)} \]
\[ \hat{\beta_0} = \hat{y_i} - \hat{\beta_1}\bar{x} \]