\documentclass[../VorlesungMaster.tex]{subfiles}

%Vorlesung 3
\[
V(X) := E[(EX)-X)^2] = E[E(X)^2 - 2X E(X) + X^2]  = E(X^2) - [E(X)]^2
\]
Bsp. Würfel
\[
V(X) = \dfrac{1}{3} \left[\left(\dfrac{5}{2}\right)^2 + \left(\dfrac{3}{2}\right)^2 + \left(\dfrac{1}{2}\right)^2\right] = \dfrac{35}{12}
\]
Bsp. Uniformverteilung [a,b]
\begin{align*}
V(X) &= \dfrac{1}{b-a} \int_a^b \! x^2 \, \mathrm{d}x - \left(\dfrac{b+a}{2}\right)^2\\
&= \dfrac{b^3 - a^3}{3 (b-a)} - \dfrac{(b+a)^2}{4} \\
&= \dfrac{(b-a) (b^2+a^2+ab}{3(b-a)} - \dfrac{(b+a)^2}{4}\\
&= \dfrac{1}{12} (4b^2+4a^2+4ab-3b^2-6 ab-3a^2)\\
&= \dfrac{1}{12} (b^2-a^2-2ab) \\
&= \dfrac{(b-a)^2}{12}
\end{align*}
\paragraph{Definition}
Wir bezeichnen $m_k$ als das \underline{gewÃ¶hnliche Moment} (oder Anfangsmoment)\\
k-ter Ordnung $m_k := E(X^k)$, \\
diskret also $\sum_i\left(x_i\right)^k p_i$,
stetig $\int \! x^k (p(x) \, \mathrm{d}x$
\\
Das \underline{zentrale Moment} (auf Zentrum Erwartungswert bezogen) k-ter Ordnung \\
$\mu_k := E\left[(X-m_1)^k\right]$. Die Varianz ist also das zweite Zentralmoment $V(X) = \mu_2 = m_2 - (m_1)^2$.\\
Man kann immer $\mu_k$ durch $m_l$ $\left( l \leq k\right)$ ausdrücken.

\subsection*{1.6 Korrelation}
\paragraph{} Eine Erweiterung dieser Momente stellt die \underline{Kovarianz}
$b(X,Y) := E\left[\left(X-E(X)\right) \left( Y-E(Y) \right)\right]$
dar (gemischte Zentralmomente zweiter Ordnung).

Es gilt offensichtlich $b(X,X) = V(X)$.\\
Die normierte GrÃ¶ße $\rho(X,Y) := \rho_{X,Y} = \dfrac{b(X,Y)}{\sqrt{V(X)V(Y)}}$ bezeichnet man als \underline{Korrelationskoeffizient}. \\
Es gilt: $-1 \leq \rho \leq 1$. Für $X=Y$ gilt $\rho = 1$ und für $X=-Y$ gilt $\rho = -1$. \\
Falls $X$ und $Y$ unabhängig dann gilt $\rho = 0$ (nicht aber umgekehrt).\\
Anwendung auf Wahrscheinlichkeiten: \\
$E(X) = p_X$, $V(X) = p_X(1 - p_X)$ \\

$\rho_{X,Y} = \dfrac{p_{XY} - p_Xp_Y}{\sqrt{p_X(1-p_X) p_Y(1-p_Y)}}$
$\rightarrow p_{XY} = p_Xp_Y + \rho_{X,Y} \sqrt{p_X(1-p_X)p_Y(1-pY)}$

\paragraph{Grenzfälle}
$\rho = 0$ : $p_{XY} = p_X p_Y$\\
$\rho = 1$\footnote{$\rightarrow p_X = p_Y$} : $p_{XY} = (p_X)^2 + p_X(1-p_X) = p_X$\\
$\rho = -1$\footnote{$\rightarrow p_X = 1 - p_Y$} : $p_{XY} = p_X(1-pX) - p_X(1-p_X) = 0 $\\

\subsection*{1.7 Einige wichtige Gesetze der Wahrscheinlichkeitstheorie}

\subsubsection*{Gesetz der großen Zahlen}
Bernoulli: Für alle $\epsilon > 0$: $\lim\limits_{n \rightarrow \infty}{P \left\lbrace| \dfrac{\mu}{n} - p | < \epsilon \right\rbrace} = 1$
\begin{itemize}
	\item $\mu$ - Anzahl der Ereignisse
	\item $n$ - Anzahl der Versuche
	\item $p$ - Wahrscheinlichkeiten der Ereignisse
\end{itemize}
%Vorlesung 4
Tschebyshew: Für ein $\epsilon >0$:
\[\lim\limits_{n \to \infty} P\{\frac{1}{n} \sum_{i=1}^{n} X_i - \frac{1}{n} \sum_{i=1}^{n} E(X_i)| < \epsilon \} = 1\] für eine Folge paarweise unabhängiger ZufallsgrÃ¶ßen.\\

$\{X_i\}_{i=1,2, \ldots n}$ mit gleichmäßiger beschränkter Varianz  $V(X_i) \leq C$


\subsubsection*{lokaler Grenzwertsatz von Mavre-Laplace}
Sei $0<p<1$ die Wahrscheinlichkeit eines Ereignisses.
In n-Versuchen gilt
\[P_n (m) = \binom{n}{m} p^m(1-p)^{n-m}\] So gilt \[\lim\limits_{n \to \infty} \frac{\sqrt{np(1-p)} P_n(m)}{\frac{1}{\sqrt{2 \pi}} e ^{-\frac{x^2}{2}}} \rightarrow  1 \text{ mit } x=\frac{m-np}{\sqrt{np(1-p)}}\]

\subsubsection*{zentraler Grenzwertsatz}

Sei $S_n =  \sum_{i=1}^{n} X_i$ mit $E(X_i)<\infty$, $V(X_i) = \sigma ^2 < \infty$

So gilt für jedes t
\[ \lim\limits_{n \rightarrow \infty} P(\frac{S_n - n E(X_i)}{\sqrt{n} \sigma}< t) = \frac{1}{\sqrt{2\pi} }\int_{-\infty}^{t} e^{\frac{-x^2}{2}} dx
\]

also: die Folge der Verteilungen der standardisierten ZufallsgrÃ¶ße konvergiert gegen die Standartnormalverteilung (d.h. a = 0, $\sigma$ = 1)


\section{Deskriptive Statistik}

\begin{itemize}
	\item Beschreibung von Daten und Kohorten ist zentral für das Verständnis einer Arbeit
	\item Ziel ist mit wenigen KenngrÃ¶ßen das wesentliche zu charakterisieren 
	\item man gibt ''Punktschätzer''  für Erwartungswerte und ''Konfidenzintervalle'' (KI; engl. CI) als Maß für die Genauigkeit der Schätzung:\\
	$(1- \alpha) \text{ KI} [a,b] : P(a \leq \theta \leq b) = 1-\alpha$
\end{itemize}

\todo{Üersichtsbild Konfidenzintervall}

\subsection{Ein Merkmal}
\subsection{nominale und Ordinale GrÃ¶ßen}
%nominal -- Kann man nur kategorisieren aber nicht ordnen
%ordinal -- kann man ist eine Kategoriale GrÃ¶ße die man in eine Reihenfolge bringen kann

\begin{itemize}
	\item absolute und relative Häufigkeiten (z.B Häufigkeitstabellen)
	\item grafisch: Balkendiagramme (mit Konfidenzintervall KI oder Standardfehler SE)
	\item Kreisdiagramm (verpÃ¶nt)
\end{itemize}

\todo{Bespiel Balkendiagramm einfügen}

$\hat{p} = \frac{r}{n}$ , $\hat{SE} = \frac{\hat{sd}}{\sqrt{n}}= \frac{\sqrt{\hat{V}}}{\sqrt{n}} = \sqrt{\frac{\hat{p} (1- \hat{p})}{n}}
KI \approx \hat{p} \pm 2 SE$, r= \#Fehlgeburten, n = \#Beobachtungen 


\subsubsection*{Metrische Daten}
\textbf{Lagemaß}
\begin{itemize}
	\item Mittelwert (arithmetisch oder geometrisch, d.h. log-Skala)
	\begin{itemize}
		\item übliche und ''robuste'' Methoden
		\item arithmetisch  = $\frac{1}{n}\sum_{i=1}^{n} X_i$
		\item geometrisch  = $[ \prod_{i=1}^{n} X_i]^{1/n}$
	\end{itemize}
	-Median und andere Qunatile (verschiedene Schätzverfahren)
\end{itemize}
\textbf{Streumaß}
\begin{itemize}
	\item Standardabweichung (''sample'' - Methode) $sd^2 = \frac{1}{n-1} \sum_{i=1}^{n}(x_i - \bar{x})^2$
	\item Interquartilabstand (engl. interquartile range (IQR) z.B. 25. und 75. Perzentil)
	\item Spannweite
	\item grafisch: Histogramm, Boxplot	
\end{itemize}
\todo{Grafik Boxplot einfügen}

\subsection{Zusammenhang 2er Merkmale}
Nominal
\begin{itemize}
	\item Kontigenztafel: odds ration, relatives Risiko
	\item grafisch: forest plots
\end{itemize}
metrisch
\begin{itemize}
	\item Korrelationskoeffizient (mit KI)
	\item Streudiagramm
\end{itemize}

\subsection{Simpsons Paradoxon}
Grundidee: Effekt in Gesamtgruppe muss nicht ''echt'' sein. Er kann in Subgruppen anders ausfallen.

\begin{tabular}{|c|c|c|}
	\hline 
	& A & B \\ 
	\hline 
	Erfolg & 70 (30\%) &  50(22\%) \\ 
	\hline 
	Misserfolg & 160 & 182 \\ 
	\hline 
	Summe & 230 & 232 \\ 
	\hline 
\end{tabular} 

\begin{tabular}{|c|c|c|c|}
	\hline 
	&   & A & B \\ 
	\hline 
	Männer & E & 7 (20\%) & 45(21\%) \\ 
	\hline 
	& M & 28 & 175 \\ 
	\hline 
	Frauen & E & 63(32\%) & 5(33\%) \\ 
	\hline 
	& M  & 132 & 10 \\ 
	\hline 
\end{tabular} 
