%Vorlesung vom 30.11.17

\paragraph{Mallows's $C_p$}
\[ C_p = \frac{1}{n}(\text{RSS} + 2k \hat{\sigma}^{2}) \]
mit $\hat{\sigma}^{2}$ als Varianzschätzer für $\epsilon_i , i \in \{1,\dots,n\}$ \\
(Voraussetzung ``Homoskedastizität'')\\
$\Rightarrow$ ``Je kleiner desto besser''
\paragraph{Akaikes Informationskriterium (AIC) }
\[
AIC := n \log (\hat{\sigma^2}) + 2k
\]
$\Rightarrow$ ``Je kleiner desto besser''

\paragraph{Bayes'sche Informationskriterium (BIC)}
\[ BIC := n log (\hat{\sigma^2}) + k \cdot log(n) \]
$\Rightarrow$ ``Je kleiner desto besser''

\paragraph{Bestimmtheitsmaß ($R^2$)}
\[ R^2 := 1 - \frac{RSS}{TSS} \]
mit 
\[ TSS := \sum\limits_{i=1}^n (y_i - \bar{y_n} )^2 \]
mit
\[ \bar{y_n} = \frac{1}{n} \sum\limits_{i=1}^{n}y_i \]
$\Rightarrow$ ``Je größer desto besser''
\underline{Bemerkung:} Man kann zeigen:
\[ 0 \leq \text{RSS} \leq \text{TSS} \]

\paragraph{Korrigiertes Bestimmtheitsmaß ($R^{2}_{adj}$)}
\[ R_{adj}^{2} := 1 - \frac{\frac{\text{RSS}}{n-k-1}}{\frac{\text{TSS}}{n-1}} \]
$\Rightarrow$ ``Je größer desto besser'' \\
\underline{Bemerkung:} RSS und $R^{2}$ hängen stark von der Anzahl der Einflussgrößen (k) ab\\
$\Rightarrow$ RSS und $R^{2}$ nur zum Vergleich von Modellen mit gleicher Anzahl von Einflussgrößen geeignet

\section{Modellwahlverfahren}
\todo[inline]{Punkt 1.2 ist Modellwahlverfahren. Evtl im Nachhinein anpassen}
\subsection{Best Subset Selection}
vergleiche alle möglichen (Sub-)Modelle miteinander
\[ \{ x^{(j_1)},\dots, x^{(j_l)}\} \subseteq \{x^{(1)}, \dots, x^{(k)} \} \]
mit 
$l \in \{1, \dots, k \} $
\[ \Rightarrow Y_i = \beta_0 + \beta_1 x_i^{(j_1)} + \dots + \beta_l x_i^{(j_l)} + \epsilon_i , i \in \{1,..n\} \]
$\Rightarrow 2^{k} $ Möglichkeiten der Modellbildung
\paragraph{Vorgehen}
\begin{enumerate}
	\item Berechne Null-Modell $\mathcal{M}_0 $:
		$Y_i= \beta_0 + \epsilon_i , i \in \{1,\dots,n\} $
	\item Für $l=1,\dots,k$:
		\begin{enumerate}
			\item Berechne alle $\binom{k}{l}$ Modell mit (genau) $l$ Einflussgrößen
			\item Wähle unter allen diesen Modellen das mit dem größten $R^{2} (\mathcal{M}_l)$
		\end{enumerate}
	\item Wähle unter $\mathcal{M}_0,\dots,\mathcal{M}_k$ das beste Modell gemäß
		\begin{itemize}
			\item kreuzvalidierter Vorhersagefehler (später)
			\item AIC
			\item BIC
			\item $R^2$
		\end{itemize}
\end{enumerate}
\underline{ABER:} Falls $k$ zu groß ist, wird Best Subset Selection zu teuer \\
Beispiel: $k=20 \Rightarrow 1.048.576$ Modelle müssen berechnet werden

\subsection{Vorwärtsselektion}
\underline{Vorgehen:} 
\begin{enumerate}
	\item Berechne das Null-Modell $\mathcal{M}_0$
	\item Für $l=0,\dots,(k-1)$
		\begin{enumerate}
			\item Berechne alle $(k-l)$ Modelle, welche das Modell $\mathcal{M}_i$ mit einem zusätzlichen Parameter bzw. Einflussgrößen betrachten
			\item Wähle unter allen diesen Modellen das mit dem größten $R^{2} (\mathcal{M}_{i+1})$
		\end{enumerate}
	\item Wähle unter $\mathcal{M}_0,\dots,\mathcal{M}_k$ das beste Modell gemäß:
		\begin{itemize}
			\item kreuzvalidierter Vorhersagefehler
			\item AIC
			\item BIC
			\item \dots
		\end{itemize}
\end{enumerate}
\underline{Vorteil:} weniger rechenintensiv:
\[ 1 + \sum\limits_{l=0}^{k-1} (k-l)=1+ \frac{k(k+1)}{2} \text{ Modelle}\]
Beispiel: $k=20 \Rightarrow 211$ Modelle müssen berechnet werden
\underline{Nachteil:} Vorwärtsselektion ``übersieht'' viele Modelle  \\
\underline{Beachte:} Falls $k > n$, ist der Algorithmus bei $l=n-1$ abzubrechen, da sonst keine eindeutigen Lösungen mehr entstehen

\subsection{Rückwärtsselektion}
\underline{Vorgehen:}
\begin{enumerate}
	\item Berechne das ``volle'' Modelle $(\mathcal{M}_k)$
	\item Für $l=k,\dots,1:$
		\begin{enumerate}
			\item Berechne alle $l$ Modelle, welche alle Einflussgrößen aus $\mathcal{M}_l$ außer einem enthalten
			\item Wähle unter allen diesen Modellen das mit dem größten $R^{2} (\mathcal{M}_{l-1})$
		\end{enumerate}
	\item Wähle unter $\mathcal{M}_0,\dots,\mathcal{M}_k$ das beste Modell gemäß:
		\begin{itemize}
			\item kreuzvalidierter Vorhersagefehler
			\item AIC
			\item BIC
			\item \dots
		\end{itemize}
\end{enumerate}
\underline{Beachte:} Hier muss $k < n$ gelten!
\section{Regularisierung (Shrinkage)}
\subsection{Ridge-Regression}
\[ \lambda > 0 \]
\[ \sum\limits_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i^{(1)} - \dots - \beta_i x_i^{(k)})^2 + \underbrace{\lambda \sum\limits_{j=1}^{k} \beta_j^{2}}_{``Strafterm''} \rightarrow min \]
	Zusatzterm sorgt dafür, dass (betragsmäßig) große $\beta_i$ vermieden werden \\
	$\lambda$ heißt \underline{Tuning-Parameter} und sterut die Stärke der Bestrafung betragsmäßig große $\beta_0 \beta_j$
	\begin{itemize}
		\item $\lambda=0$ übliche Regression mit KQM
		\item $\lambda >>0$: Bestrafung betragsmäßig großer $\beta_j$ bekommt ``Übergewicht'', das heißt $||\hat{\beta}||_2 \rightarrow 0 $ für $\lambda \rightarrow \infty $
			\[ ||\beta ||_2 := \sqrt{\sum\limits_{j=1}^{k}\beta_j^{2}} \]
	\end{itemize}
	\underline{Beobachtung:}
	\begin{itemize}
		\item bei linearer Regression hängt der Wert $\hat{\beta}_j x^{(j)}$ nicht von der Skalierung der Einflussgröße $x^{(j)}$ ab
		\item bei Ridge-Regression gilt das nicht mehr!
	\end{itemize}
$\Rightarrow$ Ausweg: Standardisierung vor der Modellberechnung gemäß:
\[ \tilde{x}_i^{(j)} := \frac{x_i^{(j)}}{\sqrt{\frac{1}{n}\sum\limits_{i=1}^{n}(x_i^{(j)} - \bar{x^{(j)}_n})}} \]
$\Rightarrow $ alle $\tilde{x^{(j)}}$ haben Varianz 1 und sind unabhängig von der Skalierung der $x^{(j)}$
$\Rightarrow$ ``Schrumpfen'' der Parameter $\hat{\beta_j}$ aber nicht ``0''
$\Rightarrow$ Reduktion der Varianz der Schätzer (``Glättung'')
